\documentclass[11pt,letterpaper]{article}
\usepackage{naaclhlt2016}
\usepackage{times}
\usepackage{latexsym}
\usepackage{xspace}

% Some helpful macros
\newcommand{\encdec}{\textsc{EncDec}\xspace}
\newcommand{\attn}{\textsc{Attn}\xspace}
\newcommand{\attncopy}{\textsc{AttnCopy}\xspace}
\newcommand{\atis}{\textsc{ATIS}\xspace}
\newcommand{\regex}{\textsc{Regex}\xspace}
\newcommand{\geo}{\textsc{Geo}\xspace}

\naaclfinalcopy % Uncomment this line for the final submission
\def\naaclpaperid{***} %  Enter the naacl Paper ID here

% To expand the titlebox for more authors, uncomment
% below and set accordingly.
% \addtolength\titlebox{.5in}    

\newcommand\BibTeX{B{\sc ib}\TeX}


\title{Compositional Data Augmentation for Semantic Parsing with Recurrent Neural Networks}
\author{Robin Jia\\
	    Computer Science Department\\
      Stanford University\\
	    {\tt robinjia@stanford.edu}
	  \And
    Percy Liang\\
    Computer Science Department\\
  	Stanford University\\
  {\tt pliang@cs.stanford.edu}}

\date{}

\begin{document}

\maketitle

\begin{abstract}
  TODO: write abstract \cite{liang2013lambdadcs}.
\end{abstract}

\section{Introduction}
TODO: write paper

\section{Related Work}
\section{Task}
Sequence-to-sequence task

Tokenization, name mangling

Overview of different datasets

\section{Model}
\subsection{Basic Model}
\subsection{Attention-based Copying}
\subsection{Learning}
During training, we maximize loglikelihood of the correct
logical form.
This strategy differs from some other semantic parsing systems
(e.g. SEMPRE), which maximize loglikelihood of the correct
denotation.  Our objective function is \[
  J.
\]
We train the model using simple stochastic gradient descent.
Gradients are computed automatically using Theano (cite).

\section{Data Augmentation}
Describe general approach.
\subsection{Augmentation for \regex}
\subsection{Augmentation for \geo}
\section{Experiments}
We evaluate our system on three domains: \atis, \regex, and \geo.
For the \atis domain, we report exact logical form match accuracy.
For \regex, we determine correctness of a predicted regular expression
by first converting it and the gold regular expression to
deterministic finite automata (DFAs).  We then call the regular expressions
equivalent if and only if corresponding automata are equivalent.
This procedure is guaranteed to call two regular expressions equivalent
if and only if they define the same regular language (CITE).
Finally, for \geo, we determine correctness based on denotation match.

We compare our full system described in section (LINK), \textbf{\attncopy}, 
to two related baselines:
\begin{itemize}
  \item \textbf{\encdec}.  An encoder-decoder LSTM model with no copying module.
  \item \textbf{\attn}.  The LSTM model with attention described in section (LINK),
     but with no copying module.
% Add third baseline of lexicon-based copying
\end{itemize}

\subsection{Implementation Details}
We ran all experiments with a hidden size of $400$ units,
and $50$-dimensional word embeddings.
We initialized all parameters uniformly at random 
within the interval $[-0.1, 0.1]$.
We used a simple learning rate schedule:
we first train the model for $25$ epochs at a learning rate of $0.1$,
then for $5$ more epochs with a learning rate of $0.05$,
and finally $5$ additional epochs with a learning rate of $0.025$.
For the \attncopy model, we replace word vectors for words
that occur only once in the training set 
with a universal \texttt{<unk>} word vector;
we do not do this for the baselines, as without the copying mechanism,
replacing rare words with \texttt{<unk>} hurts performance.

Another important hyperparameter for \regex and \geo is the
extent to which we perform data augmentation.
For \regex, we train on the original dataset,
plus all NUMBER examples generated by the integer-based scheme
and $1000$ randomly sampled new examples generated by the string-based scheme.
For \geo, we train on the original dataset,
plus $2000$ randomly sampled new examples.
All hyperparameters were tuned by training on a subset of the
training set, and evaluating on the remaining examples.

At test time, we use simple greedy decoding.
% Investigate effect of beam decoding?  It's already implemented...

\subsection{Main Results}
First, we evaluate our system trained on the original dataset alone,
and no new examples generated by our data augmentation approach.
Our results are summarized in the second group of rows of Table \ref{tab:results}.
Note that even without data augmentation, we are able to nearly
match the state-of-art on the \regex dataset.
However, we lag significantly behind on \geo.

Next, we now rerun our system and all baselines on our
augmented training datasets.  These results are shown in the
third and final group of rows of Table \ref{tab:results}.
With data augmentation, we achieve a new state-of-art level performance
on \regex, by a considerable margin.  We are also much more competitive
with state-of-art on \geo.

\begin{table}
  \centering
  \small
  \begin{tabular}{|l|c|c|c|}
    \hline
    System & \atis & \regex & \geo \\
    \hline
    Zettlemoyer 2007 & $84\%$ & & \\
    Kushman 2013 & & $67\%$ & \\
    Liang 2011 & & & $91\%$ \\
    \hline
    \textbf{Original Dataset} & & & \\
    \encdec & - & - & - \\
    \attn & - & - & - \\
    \attncopy & $78.3$ & $65.0$ & $75.0$ \\
    \hline
    \textbf{With Data Augmentation} & & & \\
    \encdec & & - & - \\
    \attn & & - & - \\
    \attncopy & & $79.0$ & $84.0$ \\
    \hline
  \end{tabular}
  \caption{Results.}
  \label{tab:results}
\end{table}

\subsection{Learning Compositionality via Alignments}
TODO: look at attention weights

We know that the attention-based copying results in
better attention alignments.
Adding the augmented data makes this even better.

\subsection{Data Augmentation and Covariate Shift}
We have shown that data augmentation can yield significant improvements
for our model, as it encourages the model to learn good alignments,
a simple form of compositionality.  Nonetheless,
there are also drawbacks to our data augmentation, as it introduces
covariate shift: certain types of examples are more likely than others
to occur in our augmented data.

To illustrate the effects of covariate shift, we ran an additional
experiment on the \regex dataset.  We trained our model
on the original dataset plus augmented data generated only from the
integer-based scheme; we also trained a separate copy of the model
on the original dataset plus augmented data generated only from the
string-based scheme.  

The results of this experiment are summarized in Table \ref{tab:regex-shift}.
We see that in isolation, each individual data augmentation scheme
does not significantly help accuracy.
The integer-based augmentation helps the model deal better
with utterances that contain an integer 
(third column of Table \ref{tab:regex-shift}),
but hurts performance on other examples.
Similarly, the string-based augmentation helps the model
deal better with utterances that contain quoted strings 
(fourth column of Table \ref{tab:regex-shift}),
but hurts performance on other examples.
Pooling both sources of augmented data improves performance
across the board.

\begin{table}
  \centering
  \small
  \begin{tabular}{|l|c|c|c|}
    \hline
    Dataset & Accuracy & Acc. on Int & Acc. on String \\
    \hline
    Original & $65.0$ & $11/19$ & $54/82$ \\
    Int-Augmented & $67.0$ & $15/19$ & $52/82$ \\
    String-Augmented & $65.0$ & $8/19$ & $56/82$\\
    All Augmented & $79.0$ & $17/19$ & $62/82$ \\
    \hline
  \end{tabular}
  \caption{Results.}
  \label{tab:regex-shift}
\end{table}

\section{Discussion}

\section*{Acknowledgments}

Do not number the acknowledgment section.

\bibliography{all}
\bibliographystyle{naaclhlt2016}


\end{document}
